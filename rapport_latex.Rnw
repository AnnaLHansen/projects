\documentclass{report}

\usepackage[utf8]{inputenc}

\begin{document}
\SweaveOpts{concordance=TRUE}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
 
        \Huge
        \textbf{Machine learning - DTU}
 
        \vspace{0.5cm}
        \LARGE
        Rapport 
 
        \vspace{1.5cm}
 
        \textbf{Anna Louise Hansen}
        \vfill
 
        Fra Udviklings- og Forenklingsstyrrelsen
 
        \vspace{0.8cm}
 
    \end{center}
\end{titlepage}

<<echo=FALSE>>=
library(dplyr)
library(knitr)
library(corrplot)
setwd("~/projects")
source("help_func_datainspec.R")
source("help_func_standardization.R")
@


\chapter{Part I}

\section{Beskrivelse af data}
Alle boligejere i Danmark betaler en skat, ejendomsværdiskat, som er baseret på værdien af deres ejendom. Dette vil sige hele ejendommen inkl. grunden som boligen ligger på. For at kunne gøre dette laver den danske stat offentlige ejendomsvurderinger som disse skatter bliver baseret på. 
Det er derfor vigtigt at disse vurderinger er retvisende og ikke mindst forklarbare, således at en borger kan forstå hvilke parametre der ligger til grund for ejendomsvurderingen. 
Til dette project har jeg valgt at arbejde med anonymiseret data fra mit arbejde i udviklings- og forenklingsstyrrelsen, hvor jeg til dagligt arbejder med netop dette. Datasættet består af ejendomssalg fra en 6 årig periode. Ud over selve huspriserne består data også af en lang række attributter som beskriver karakteristika ved selve boligen. Det kan f.eks. være tagmateriale, boligens opførelsesår, information om størrelsen af huset og grunden eller bbr koder som dækker over boligens anvendelse. 
Der ud over består data også af en lang række attributter som fortæller noget om hvor boligens beliggenhed. Det kan f.eks. være boligens koordinater eller information om afstanden til kyst og skov eller afstand til motorvej og jernbane. 
Data kommer fra en række forskellige registre og offentlige styrrelser som eks. BBR og Styrrelsen for Dataforsyning og Effektivisering.

Til dette projekt vil jeg overordnet set prøve at se hvor godt man kan forudsige ejendomsværdier ud fra salgspriserne fra en 6-årig periode. 

Jeg vil med Principal Component Analysis få et overblik over de data der er til rådighed og få et visuelt overblik over attributterne. 
Herefter vil jeg med en unsupervised learning forsøge at gruppere det data jeg har til at generer yderlige attributer som kan indgå i modellen. Jeg vil her specifikt prøve at se om det er muligt at gruppere salgene i forskellige boligtyper. Jeg vil i samme omgang også forsøge at frasorterer outliers i data med anomaly detection. 
Herefter vil jeg med regressions model forsøge at kaste lys over projektets overordnede problem ved at forsøge at forudsige huspriserne ud fra salgspriser. I tilfælde af at modellen ikke ikke kan komme med en god prædiktion af en given ejendom vil det være muligt at denne ejendom bliver manuelt værdiansat af en sagsbehandler. Jeg vil derfor til slut med en classifikation forsøge at estimerer om en ejendom skal ud til manuel sagsbehandling baseret på dens estimerede ejendomsværdi.


\section{Detaljeret beskrivelse af data}

<<echo=FALSE>>=
train <- readRDS("anonym_data_kursus.rds")
@


Det salgsdata som jeg har valgt at arbejde med dækker i udgangspunktet `r nrow(train)` observationer med `r ncol(train)` attributter. Inden jeg går i gang med at kigge på data har jeg valgt at lave en oprydning i data. Det har jeg gjort fordi mange af attributterne bliver i mit daglige arbejde brugt i forbindelse med imødekomme diverse forretningskrav. Desuden dækker observationerne mange forskellige typer af ejendomssalg. Det er en blanding af parcelhussalg, rækkehussalg, sommerhussalg, salg af ejerlejligheder mm. og ud fra et forretningsmæssigt perspektiv giver det ikke mening at træne en model på alle salg og ejendomstypen vil påvirke salgsprisen. F.eks. vil der på sommerhuse være restriktioner på hvor meget om året man må bo i sommerhuset og der kan være i sommerhusområder være andre regler for hvad man må bruge sin grund til end der er i et parcelhusområde. Jeg har derfor ligeledes valgt at reducerer antallet af observationer således at de kun dækker almindelige parcelhus. Dette er gjort ved kun at beholde alle de ejendomssalg, hvor ejendommen i BBR er registreret med enheds- og bygningsanvendelsen 120. 

<<echo=FALSE>>=
train <- readRDS("train_klargjort_salg.rds")
@

Herefter er der `r nrow(train)` observationer tilbage og `r ncol(train)` attributter.

Der er for en del af attributterne en stor andel af manglende værdier. Det er især i forhold til variable fra BBR, som beskriver forskellige karakteristika ved selve boligen. Her har jeg har valgt at fjerne alle de attributter som har mere end 95\% manglende værdier.

<<echo=FALSE>>=
attribut_oversigt <- readRDS("atrtribut.rds")
attribut_oversigt <- attribut_oversigt[order(attribut_oversigt$pct_missing_values, decreasing = TRUE),]
knitr::kable(attribut_oversigt)
@

Som udgangspunkt vil jeg træne en model som skal være i stand til at kunne prædiktere værdien af et standard
parcelhus. Data som der skal trænes på skal derfor også være salg af standard parcelhuse.
Data er derfor blevet ensrettet på følgende måde:

% - enhed.antalvaerelser > 1 og enhed.antalvaerelser < 10
% - bolig_areal < 500 og bolig_areal > 50
% - bolig_alder > 0 og bolig_alder < 100
% - bygning.antaletager > 0 og bygning.antaletager < 4
% - enhed.antalbadevaerelser > 0 og enhed.antalbadevaerelser < 4
% - enhed.antalvandskylledetoiletter > 0 og enhed.antalvandskylledetoiletter < 4
% - fremskreven_pris_M2 < 30000 og fremskreven_pris_M2 > 0

Slutteligt er der blevet taget et valg om kun at udvælge de rå attributter som menes at være de
vigtigste i forhold til at forudsige værdien af et standad parcelhus.

<<echo=FALSE>>=
train <- readRDS("train_behandlet.rds")
attribut <- undersoeger_attributter(train)
knitr::kable(attribut)
@

 
De to attributter der dækker over tagtypematriale og ydervægsmateriale er diskrete variable som fordel
kan normaliseres med en one-out-of-k transformering.

Afstand til kyst og afstand til motorvej er to variable som jeg har valgt at binariserer. Det har jeg da de to
features forud for denne rapport er blevet imputeret. For afstand til kyst er afstanden op til 1500 meter målt.
Alt herover er imputeret til 1501 meter. Ligeledes er gjort for afstand til motorvej.
For at håndtere disse variable har jeg som sagt valgt at binariserer dem. For afstand til kyst har jeg ud fra et
forretningsmæssigt synspunkt valgt at en ejendom ligger så tæt på kysten at det har en effekt i dens værdi hvis
den ligger inden for 300 meter af kysten. Det samme er gjort for afstand til vej. Her er grænsen dog blot 100
meter.


<<echo=FALSE>>=
train <- one_out_of_k(train)
train <- binarisere_afstande(train)
@

\section{Data visualisering heriblandt Principal Component Analysis (PCA)}

Principal Component Analysis (PCA) er en metode som kan bruges til at reducere dimensionerne i det data der arbejdes med. Man kan have mange dimensioner i sit datasæt, men hvis de alle sammen er med til at forklare sammen tendens er det 'sande' antal af dimensioner lavere end antallet af attributter. Målet med at lave PCA er at reducere dimensionerne i data uden at reducere variationen, således at man ender op med data som med færre dimensioner men som kan forklare det samme som datasættet med flere dimensioner. PCA fungerer kun ud fra antagelsen om at der er en linear forklaring i data med færre dimensioner. De bedste projektion af data ned på et subspace er dem hvor observationer er spredt ud (Høj varians), men samtidig hvor residualerne reduceres. 
Vektoren bliver valgt ud fra at den skal være en eigenvektor til vores datamatrix som har den højeste eigenværdi. Singular Value Decomposition (SVD) er en metode som for en hvilken som helst $N*M$ matrix udregner eigenvektoren med den højeste eigenværdi.  

Ejendomsdata er blevet klargjort. Data er blevet tranformeret. Nogle variable er blevet tranformeret med one-out-of-K transformation, mens enkelte er blevet binariseret. 
Til PCA er det første trin at standardisere data, således at attributternes værdier er på samme skala. Selve standardiseringen består i at trække gennemsnittet fra hver attribut, hvorefter der også er blevet divideret med standardafvigelsen. For data betyder det at hver attribut reskaleres således at de får et gennemsnit på 0 og en standardafvigelse på 1. 
Årsagen til at en reduktion af dimensionerne er ønskværdig er at det for nogle typer af algoritmer kan være med til at forøge deres nøjagtighed. Dette er eksempelvis tilfældet med xgboost algoritmen. 

Efter alle datatransformationerne består data af 185018 observationer (N) med 23 features (M).
Dette data skal senere danne grundlaget for regressionsanalysen, men inden da bliver der med en korrelationsanalyse og en PCA taget stilling til hvorvidt det er muligt at reducere demensionerne i data. 
Resultatet af korrelationsanalysen er vist i et korrelationsplot. Resultatet af Korrelationsanalysen viser at der er en stor positiv korrelation mellem den fremskrevne kvadratmeter pris og den vægtede gennemsnitspris for de nærmeste naboer. Der er desuden også an større positiv sammenhæng mellem antallet af værelser og boligarealet. 
Disse to positive sammenhænge giver logik rigtig god mening. Salgspriser er i høj grad styret af det område som ejendommen ligger i. Ligger ejendommen i et dyrt område, vil naboerne blive solgt til høje handelspriser og det samme vil højst sandsynligt også gælde for den specifikke ejendom. 
Samtidig vil der typisk også være flere væresler jo større boligareal en ejendommene har. 



<<echo=FALSE, fig=TRUE>>=
train <- standardize(train)
correlation <- cor(train)
print(colnames(correlation))
rownames(correlation) <- 1:23
colnames(correlation) <- 1:23
corrplot(correlation)
@

Som en del af PCA udregnes herefter Singular Value Decomposition (SVD).


<<echo = FALSE, fig=TRUE>>=
# Lave singular value decomposition analyse:
svd <- svd(train)
svd_u <- svd$u
svd_v <- svd$v
svd_sigma <- diag(svd$d)
eigenvalues <- diag(svd_sigma^2/(nrow(train) - 1))
variance <- eigenvalues * 100 /sum(eigenvalues)
cumvar <- cumsum(variance)

plot(variance,
     type = "o",
     pch = 16,
     main = "Varians forklaret med principal components",
     xlab = "Principal components",
     ylab = "Forkalret varians",
     col = "deepskyblue")
@

<<echo = FALSE, fig=TRUE>>=
plot(cumvar,
     type = "o",
     pch = 16,
     main = "Varians forklaret med principal components",
     xlab = "Principal components",
     ylab = "Kumulativ varians",
     col = "darkorchid")
@


De foeste 16 principal components kan forklare 90\% af variationen i data. For at kommme over 95\% skal man have de 18 første komponenter. Ud af de i alt 23 mulige komponenter er det med dette data ikke muligt at reducerer mange komponenter væk uden også at miste variation i data. 
Ved at have antallet af komponenter som er mindre end antallet af attributter i ens datasæt bliver information tabt, og hvorvidt man med fordel kan bruge PCA skal bestemmes ud fra den pågældende problemstilling. I den videre opgave har jeg valgt at gå videre med mit originale datasæt som det så ud før PCA. 

\chapter{Part II}

\section{Regression - part A}
I part II er formålet at bruge det rensede data fra part I til at forudsige fremskrevne handelspriser ud fra forskellige varibable. 
Håbet med denne regressionsanalyse er at man ud fra relativt få variable og en relativt simpel model vil kunne forudsige ejendomspriserne. 
Forud for regressionsanalysen er data blevet tranformeret. For faktorvariablene tagtype og vægmateriale har jeg valgt at tranformere med en one-of-k transformering. Herefter er alle attributer blevet standardiseret, således at de har en gennemsnit på 0 og en standardafvigelse på 1. 

Den første lineære model der fittes er en univariate linear regressionsmodel. Her er naboernes områdepris den eneste variabel som bruges til at forudsige de fremskrevne handelspriser. 
Denne simple lineaære regression er vist i figuren nedenfor. 

<<echo=FALSE, fig=TRUE>>=
# Estimate model parameters
w_est = lm(fremskreven_pris_M2 ~ EV_NN_M2, data = train)
# Plot the predictions of the model
plot(train$fremskreven_pris_M2, train$EV_NN_M2,
     main='Simpel lineær regression',
     xlab="Naboernes kvadratmeterpriser", 
     ylab="Fremskrevne kvadratmeterpriser");
y_est = w_est$coef[1] +w_est$coef[2]*train$fremskreven_pris_M2;
lines(train$fremskreven_pris_M2, y_est, col='red');
legend("topleft", legend=c("Data", "Fittet model"), fill=c("black", "red"))
@

Den anden lineære model der fittes er en multivariate lineær regressionsmodel. 
Variablene som bliver brugt i modellen er områdepriser i form af naboernes kvadratmeterpriser. Det er boligens opførelsesalder og ombygningsår, og det er boligens og grundens areal. 
I en multivariate lineær regressionsmodel kan man ikke på samme måde plotte den fittede model på det todimensionelle plot. Her kan man i stedet estimerer vide hvor godt modellen fitter til data ved akan man ved at minimere summen af de kvadrerede afvigelser (RSS). 
Der findes flere forskellige typer af algoritmer hvis formål er at finde de parametre/vægte som laver det bedste fit til data ved at minimerer 'cost'. 


<<echo=FALSE, fig=TRUE>>=
# Estimate model parameters
fit = lm(fremskreven_pris_M2 ~ EV_NN_M2 + bolig_areal + aux.ice_info.jordstykker.registreretareal_fratrukket_vejareal + bolig_alder + bolig_alder, data = train)

train$predicted <- predict(fit)   # Save the predicted values
train$residuals <- residuals(fit) # Save the residual values

train %>% select(fremskreven_pris_M2, predicted, residuals) %>% head()

#Plot the predictions of the model
par(mfrow = c(2, 2))
plot(fit)

print(summary(fit))

# Make a scatter plot of predicted versus true values of Alcohol
plot(train$fremskreven_pris_M2, fit$fitted.values, 
     xlab='fremskreven_pris (true)', 
     ylab='fremskreven_pris (estimated)',
     main='salgspriser',
     pch=16,
     col = "deepskyblue")

# Make a histogram of the residual error
hist(train$fremskreven_pris_M2-fit$fitted.values,
     breaks=41,
     main="Residual error")

# Compute mean squared error
mean((train$fremskreven_pris_M2-fit$fitted.values)^2)
@

Evalueringen af den lineære model kan foregå ved at man træner på et datasæt og prædikterer på et andet. I følgende regression har jeg nabokvadratmeterpriserne og boligarealet med som prediktorvariable. 
Ud fra den estimerede model kan man plotte sin afhængige variabel mod de predikterede værdier.
Forskellen mellem den faktiske afhænge variabel (i dette tilfælde de fremskrevne salgspiser) og den prædikterede variabel (modellens estimerede y) er residualet.
Selve den lineære regressionsmodel kan anvanceres ved at tranformere inputvariablene. Det kan eksempelvis være at opløfte variablene i anden potens og bruge dem som prediktor sammen med den originale version af variablen.
Målet for denne simple lineære regressionsmodel er reducere 'cost-funktionen' så meget som muligt. For en simpel lineær regressiosmodel er cost-funktionen en squared error, hvilket vil sige at målet her er at reducere 'mean squared error' så meget som muligt - dog uden at overfitte. 
Andre cost-funktioner kan bruges til andre problemstillinger. En logistisk funktion kan således bruges til at hvis man arbejder med et logistisk regressionsproblem. 

En af de større udfordringer der er indenfor machine learning er 'underfitting' og 'overfitting'. 'Underfitting' er når en model ikke fanger de tendenser som er i data - at modellen er for simpel. 'Overfitting' derimod er når modellen fanger alle små variationer i det data den er trænet på og at den på den måde fanger trends som ikke er der. Det kan ses når man anvender modellen på et nyt datasæt, hvor den i tilfældet af 'overfitting' vil performe dårligt. Overfitting kan forekomme hvis antallet af features er meget højt i forhold til antallet af observationer. 
Ridge regression og regularisering - ridge regression er regressionsmetode som bruges for at undgå at man overfitter der burger regularisering. Regularisering er er tilpasning der laves til algoritmen som reducerer generalization error ved at bruge en regulariseringsparameter (lambda). Hvis lambda sættes til 0, svarer til det til at modellen ikke bliver regulereret mens en høj lambda reducerer kompleksiteten i modellen.

<<echo=FALSE>>=
# x1 <- train[1:80000, ] 
# x2 <- train[80000:185018, ]
# 
# dat <- data.frame(x1[, c("EV_NN_M2","bolig_areal")])
# xnam <- paste("X", 1:2, sep="")
# colnames(dat) <- xnam
# y <- train$fremskreven_pris_M2[1:80000]
# (fmla <- as.formula(paste("y ~ ", paste(xnam, collapse= "+"))))
# w_est = lm(fmla, data=dat);
# 
# # Plot the predictions of the model
# plot(train$EV_NN_M2, train$fremskreven_pris_M2, main="Linear regression", xlab="X", ylab="y")
# 
# newdat <- data.frame(x2[, c("EV_NN_M2","bolig_areal")])
# colnames(newdat) <- xnam
# y_est = predict(w_est, newdata=newdat)
# lines(newdat[,1], y_est, col='red')
# legend("topleft", legend=c('Data', 'Fitted model'), fill=c("black", "red"))
@


<<echo=false, fig=TRUE>>=
dat <- readRDS("train_behandlet_og_standardz.rds")
X <- dat[, colnames(dat) != "fremskreven_pris_M2"]
N <- nrow(X)
attributeNames <- colnames(X)
M <- ncol(X)
y <- dat[, colnames(dat) == "fremskreven_pris_M2"]
T <- 14 # antallet af afpøvede lambdaer

# Regularized Linear regression 
# include an additional attribute corresponding to the offset
X <- cbind( rep(1,N), X) # tilfoejer 1-taller til matrixen
M <- M[1]+1
attributeNames <- c("Offset",attributeNames)

# Crossvalidation
# Create crossvalidation partition for evaluation of performance of optimal model
K <- 5
set.seed(1234) # for reproducibility
CV <- cvFolds(N, K=K)
# set up vectors that will store sizes of training and test sizes
CV$TrainSize <- c()
CV$TestSize <- c()

# values of lambda
lambda_tmp <- 10^(-5:8)

# Initialise variables
KK <- 10 # inner loop
temp <- rep(NA, M*T*KK); 
w <- array(temp, c(M, T, KK));  

Error_train2 <- matrix(rep(NA, times=T*KK), nrow = T)
Error_test2 <- matrix(rep(NA, times=T*KK), nrow = T)
lambda_opt <- rep(NA, K)
w_rlr <- matrix(rep(NA, times=M*K), nrow=M)
Error_train_rlr <- rep(NA,K)
Error_test_rlr <- rep(NA,K)
w_noreg <- matrix(rep(NA, times=M*K), nrow=M)
mu <- matrix(rep(NA, times=(M-1)*K), nrow=K)
sigma <- matrix(rep(NA, times=(M-1)*K), nrow=K)
Error_train <- rep(NA,K)
Error_test <- rep(NA,K)
Error_train_nofeatures <- rep(NA,K)
Error_test_nofeatures <- rep(NA,K)


for(k in 1:K){
  
  paste('Crossvalidation fold ', k, '/', K, sep='')
  # Extract the training and test set
  X_train <- X[CV$subsets[CV$which!=k], ];
  y_train <- y[CV$subsets[CV$which!=k]];
  X_test <- X[CV$subsets[CV$which==k], ];
  y_test <- y[CV$subsets[CV$which==k]];
  CV$TrainSize[k] <- length(y_train)
  CV$TestSize[k] <- length(y_test)
  
  # Use 10-fold crossvalidation to estimate optimal value of lambda    
  KK <- 10
  
  CV2 <- cvFolds( dim(X_train)[1], K=KK)
  CV2$TrainSize <- c()
  CV2$TestSize <- c()
  
  for(kk in 1:KK){
    
    # traekker herefter også traenings og test saettet ud 
    # til den 'indre' krydsvalidering som bruges til at bestemme 
    # den optimale lambda
    X_train2 <- X_train[CV2$subsets[CV2$which!=kk], ]
    y_train2 <- y_train[CV2$subsets[CV2$which!=kk]]
    X_test2 <- X_train[CV2$subsets[CV2$which==kk], ]
    y_test2 <- y_train[CV2$subsets[CV2$which==kk]]
    
    # standardisere inde i det indre loop - Derfor bruge funktionen scale
    mu2 <- colMeans(X_train2[, 2:ncol(X_train2)])
    sigma2 <- apply(X_train2[, 2:ncol(X_train2)], 2, sd)
    
    # hvad laves her? scales er en funktion centrerer kolonnnerne i en matrice
    X_train2[, 2:ncol(X_train2)] <- scale(X_train2[, 2:ncol(X_train2)], mu2, sigma2)
    X_test2[, 2:ncol(X_train2)] <- scale(X_test2[, 2:ncol(X_train2)], mu2, sigma2)
    
    CV2$TrainSize[kk] <- length(y_train)
    CV2$TestSize[kk] <- length(y_test2)
    
    Xty2 <- t(X_train2) %*% y_train2
    X_train2 <- as.matrix(X_train2)
    XtX2 <- t(X_train2) %*% X_train2
    
    for(t in 1:length(lambda_tmp)){
      
      # Learn parameter for current value of lambda for the given inner CV_fold
      lambdaI = lambda_tmp[t]*diag(M);
      lambdaI[1,1] = 0; # don't regularize bias
      w[,t,kk] <- solve(XtX2+lambdaI) %*% Xty2
      
      # Evaluate training and test performance 
      Error_train2[t,kk] = sum((y_train2 - X_train2 %*% w[,t,kk])^2)
      X_test2 <- as.matrix(X_test2)
      Error_test2[t,kk] = sum((y_test2 - X_test2 %*% w[,t,kk])^2)
      
    }
  }
  
  # Display result for cross-validation fold
  w_mean <- apply(w, c(1,2), mean)
  
  # Plot weights as a function of the regularization strength (not offset)
  par(mfrow=c(1,2))
  plot(log(lambda_tmp), w_mean[2,], xlab="log(lambda)",
       ylab="Coefficient Values",main=paste("Weights, fold ",k,"/",K),
       ylim = c(min(w_mean[-1,]), max(w_mean[-1,])))
  lines(log(lambda_tmp), w_mean[2,])
  
  colors_vector = colors()[c(1,50,26,59,101,126,151,551,71,257,506,634,639,383)]
  
  for(i in 3:M){
    points(log(lambda_tmp), w_mean[i,], col=colors_vector[i])
    lines(log(lambda_tmp), w_mean[i,], col=colors_vector[i])
  }
  
  #matplot(log(lambda_tmp), t(w_mean), type = c("b"),pch=1,col = 1:4) #plot
  
  # Select optimal value of lambda
  ind_opt <- which.min(apply(Error_test2, 1, sum) / sum(CV2$TestSize))
  lambda_opt[k] <- lambda_tmp[ind_opt]
  
  
  par(cex.main=3) # Define size of title
  par(cex.lab=2) # Define size of axis labels
  par(cex.axis=2) # Define size of axis labels
  par(mar=c(5,6,4,1)+.1) # Increase margin size to allow for larger axis labels
  plot(log(lambda_tmp), log(apply(Error_train2,1,sum)/sum(CV2$TrainSize)), 
       xlab="log(lambda)", ylab="log(Error)" , 
       main = paste("Otimal lambda: 1e",log10(lambda_opt[k])))
  
  lines(log(lambda_tmp), log(apply(Error_train2,1,sum)/sum(CV2$TrainSize)))
  
  points(log(lambda_tmp), log(apply(Error_test2,1,sum)/sum(CV2$TestSize)) ,col="red")   
  lines(log(lambda_tmp), log(apply(Error_test2,1,sum)/sum(CV2$TestSize)) , col="red")
  
  legend("bottomright", legend=c("Training","Test"), col=c("black","red"), lty=1)
  
  # Standardize outer fold based on training set, and save the mean and standard
  # deviations since they're part of the model (they would be needed for
  # making new predictions) - for brevity we won't always store these in the scripts
  mu[k,] <- colMeans(X_train[, 2:ncol(X_train)])
  sigma[k,] <- apply(X_train[, 2:ncol(X_train)], 2, sd)
  
  X_train[, 2:ncol(X_train)] <- scale(X_train[, 2:ncol(X_train)], mu[k,], sigma[k,])
  X_test[, 2:ncol(X_test)] <- scale(X_test[, 2:ncol(X_test)], mu[k,], sigma[k,])
  
  # Estimate w for the optimal value of lambda
  Xty = t(X_train) %*% y_train
  X_train <- as.matrix(X_train)
  XtX = t(X_train) %*% X_train
  
  lambdaI = lambda_opt[k] * diag(M)
  lambdaI[1,1] = 0; # don't regularize bia
  
  w_rlr[,k] = solve(XtX+lambdaI) %*% Xty
  
  X_test <- as.matrix(X_test)
  # evaluate training and test error performance for optimal selected value oflambda
  Error_train_rlr[k] = sum( (y_train - X_train %*% w_rlr[,k])^2 )
  Error_test_rlr[k] = sum( (y_test - X_test %*% w_rlr[,k])^2 )
  
  # Compute squared error without regularization
  w_noreg[,k] = solve(XtX) %*% Xty
  Error_train[k] = sum( (y_train - X_train %*% w_noreg[,k])^2);
  Error_test[k] = sum( (y_test - X_test %*% w_noreg[,k])^2);
  
  # Compute squared error without using the input data at all
  Error_train_nofeatures[k] = sum((y_train - mean(y_train))^2);
  Error_test_nofeatures[k] = sum((y_test - mean(y_train))^2);
  
}

# Display Results
print('Linear regression without feature selection:');
print(paste('- Training error: ', sum(Error_train)/sum(CV$TrainSize)));
print(paste('- Test error', sum(Error_test)/sum(CV$TestSize)));
print(paste('- R^2 train:     %8.2f\n', (sum(Error_train_nofeatures)-sum(Error_train))/sum(Error_train_nofeatures)))
print(paste('- R^2 test:     %8.2f\n', (sum(Error_test_nofeatures)-sum(Error_test))/sum(Error_test_nofeatures)))

print('Regularized Linear regression:')
print(paste('- Training error:', sum(Error_train_rlr)/sum(CV$TrainSize)))
print(paste('- Test error:', sum(Error_test_rlr)/sum(CV$TestSize)))
print(paste('- R^2 train: ', (sum(Error_train_nofeatures)-sum(Error_train_rlr))/sum(Error_train_nofeatures)))
print(paste('- R^2 test:', (sum(Error_test_nofeatures)-sum(Error_test_rlr))/sum(Error_test_nofeatures)))

print('Weights in last fold :')
for(m in 1:M){
  print(paste(attributeNames[m], w_rlr[m, k]))
  
}

@

\section{Regression - part B}

Kunstige neurale netværk (ANN) er en metode indenfor machine learning som er et sæt af algoritmer som er designet til at genkende mønstre. I dette tilfælde med ejendomspriser fodres machine learning algoritmen med en masse features som der ud fra et forretningsmæssigt synspunkt har betydning for priserne samt de tilhørende ejendomspriser. Machine learning algoritmen tager disse observationer som med sin læringsalogritme lærer at fordusige ejendomspriserne ud fra de features den bliver givet. 
Den mest simple form for et kunstigt neuralt netværk er et 'feedforward' netværk. I ANNs betrages som lag af neuroner som videregiver information fra et lag til det næste. Det yderste lag i et ANN er inputlaget som er de features der bliver fodret ind i modellen. Ud fra et sæt af vægte bliver denne information givet videre til de næste underliggende lag ('forward pass'). De næste lag kan bestå af et eller flere skjulte lag og til sidst har man det yderste lag som er output laget. 
Et neuralt netværk kan være fra inputlaget med input features reducere antallet af dimensioner til outputlaget, hvilket gør dem gode til at løse multidemensionelle regressions- og klassifikationsproblemer. 
I det/de skjulte lag bliver hver at skjulte enheder transformeret med en ikke lineær aktiveringsfunktion. Til træning af dette neurale netværk er det aktiveringsfunktionen "tahn" der er blevet anvendt. 

<<>>=
library(neuralnet) #install.packages("neuralnet")
library(cvTools)

# Load data
dat <- readRDS("train_behandlet_og_standardz.rds")
X <- dat
N <- nrow(X)
attributeNames <- colnames(X)
M <- ncol(X)
y <- dat[, colnames(dat) == "fremskreven_pris_M2"]

# K-fold crossvalidation
K = 10;
set.seed(1234) # for reproducibility
CV <- cvFolds(N, K=K)
# set up vectors that will store sizes of training and test sizes
CV$TrainSize <- c()
CV$TestSize <- c()

# Parameters for neural network classifier
NHiddenUnits = 2;  # Number of hidden units
NTrain = 1; # Number of re-trains of neural network

# Variable for classification error
Error = rep(NA, times=K)
(fmla <- as.formula(paste("y_train ~ ", paste(attributeNames, collapse= "+"))))
for(k in 1:K){ # For each crossvalidation fold
        print(paste('Crossvalidation fold ', k, '/', K, sep=''))

    # Extract training and test set
    X_train <- X[CV$subsets[CV$which!=k], ];
    y_train <- y[CV$subsets[CV$which!=k]];
    X_test <- X[CV$subsets[CV$which==k], ];
    y_test <- y[CV$subsets[CV$which==k]];
    CV$TrainSize[k] <- length(y_train)
    CV$TestSize[k] <- length(y_test)
        
    X_traindf <- data.frame(X_train)
    colnames(X_traindf) <- attributeNames
    X_testdf <- data.frame(X_test)
    colnames(X_testdf) <- attributeNames
        
    # Fit neural network to training set
    MSEBest = Inf;
    for(t in 1:NTrain){
        netwrk = neuralnet(fmla, X_traindf, hidden=NHiddenUnits, act.fct='tanh', linear.output=TRUE, err.fct='sse');
        mse <- sum((unlist(netwrk$net.result)-y_train)^2)

        if(mse<MSEBest){
          bestnet <- netwrk
          MSEBest <- mse
        }
      }
            # Predict model on test data
        
        computeres <- compute(bestnet, X_testdf)
    y_test_est = unlist(computeres$net.result)
    
    # Compute error rate
    Error[k] = sum((y_test-y_test_est)^2); # Count the number of errors

      }
    

# Print the error rate
print(paste('Mean Sum of Squares Error (MSSE): ', sum(Error)/sum(CV$TestSize), sep=''));

# Display the trained network (given for last cross-validation fold)
plot(bestnet);

@

\section{Classification}

\chapter{Part III}




\end{document}