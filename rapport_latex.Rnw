\documentclass{report}

\usepackage[utf8]{inputenc}

\begin{document}
\SweaveOpts{concordance=TRUE}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
 
        \Huge
        \textbf{Machine learning - DTU}
 
        \vspace{0.5cm}
        \LARGE
        Rapport 
 
        \vspace{1.5cm}
 
        \textbf{Anna Louise Hansen}
        \vfill
 
        Fra Udviklings- og Forenklingsstyrrelsen
 
        \vspace{0.8cm}
 
    \end{center}
\end{titlepage}

<<echo=FALSE>>=
library(dplyr)
library(knitr)
library(corrplot)
setwd("C:/Users/alhan/projects")
source("help_func_datainspec.R")
source("help_func_standardization.R")
@


\chapter{Part I}

\section{Beskrivelse af data}
Alle boligejere i Danmark betaler en skat, ejendomsværdiskat, som er baseret på værdien af deres ejendom. Dette vil sige hele ejendommen inkl. grunden som boligen ligger på. For at kunne gøre dette laver den danske stat offentlige ejendomsvurderinger som disse skatter bliver baseret på. 
Det er derfor vigtigt at disse vurderinger er retvisende og ikke mindst forklarbare, således at en borger kan forstå hvilke parametre der ligger til grund for ejendomsvurderingen. 
Til dette project har jeg valgt at arbejde med anonymiseret data fra mit arbejde i udviklings- og forenklingsstyrrelsen, hvor jeg til dagligt arbejder med netop dette. Datasættet består af ejendomssalg fra en 6 årig periode. Ud over selve huspriserne består data også af en lang række attributter som beskriver karakteristika ved selve boligen. Det kan f.eks. være tagmateriale, boligens opførelsesår, information om størrelsen af huset og grunden eller bbr koder som dækker over boligens anvendelse. 
Der ud over består data også af en lang række attributter som fortæller noget om hvor boligens beliggenhed. Det kan f.eks. være boligens koordinater eller information om afstanden til kyst og skov eller afstand til motorvej og jernbane. 
Data kommer fra en række forskellige registre og offentlige styrrelser som eks. BBR og Styrrelsen for Dataforsyning og Effektivisering.

Til dette projekt vil jeg overordnet set prøve at se hvor godt man kan forudsige ejendomsværdier ud fra salgspriserne fra en 6-årig periode. 

Jeg vil med Principal Component Analysis få et overblik over de data der er til rådighed og få et visuelt overblik over attributterne. 
Herefter vil jeg med en unsupervised learning forsøge at gruppere det data jeg har til at generer yderlige attributer som kan indgå i modellen. Jeg vil her specifikt prøve at se om det er muligt at gruppere salgene i forskellige boligtyper. Jeg vil i samme omgang også forsøge at frasorterer outliers i data med anomaly detection. 
Herefter vil jeg med regressions model forsøge at kaste lys over projektets overordnede problem ved at forsøge at forudsige huspriserne ud fra salgspriser. I tilfælde af at modellen ikke ikke kan komme med en god prædiktion af en given ejendom vil det være muligt at denne ejendom bliver manuelt værdiansat af en sagsbehandler. Jeg vil derfor til slut med en classifikation forsøge at estimerer om en ejendom skal ud til manuel sagsbehandling baseret på dens estimerede ejendomsværdi.


\section{Detaljeret beskrivelse af data}

<<echo=FALSE>>=
train <- readRDS("anonym_data_kursus.rds")
@


Det salgsdata som jeg har valgt at arbejde med dækker i udgangspunktet `r nrow(train)` observationer med `r ncol(train)` attributter. Inden jeg går i gang med at kigge på data har jeg valgt at lave en oprydning i data. Det har jeg gjort fordi mange af attributterne bliver i mit daglige arbejde brugt i forbindelse med imødekomme diverse forretningskrav. Desuden dækker observationerne mange forskellige typer af ejendomssalg. Det er en blanding af parcelhussalg, rækkehussalg, sommerhussalg, salg af ejerlejligheder mm. og ud fra et forretningsmæssigt perspektiv giver det ikke mening at træne en model på alle salg og ejendomstypen vil påvirke salgsprisen. F.eks. vil der på sommerhuse være restriktioner på hvor meget om året man må bo i sommerhuset og der kan være i sommerhusområder være andre regler for hvad man må bruge sin grund til end der er i et parcelhusområde. Jeg har derfor ligeledes valgt at reducerer antallet af observationer således at de kun dækker almindelige parcelhus. Dette er gjort ved kun at beholde alle de ejendomssalg, hvor ejendommen i BBR er registreret med enheds- og bygningsanvendelsen 120. 

<<echo=FALSE>>=
train <- readRDS("train_klargjort_salg.rds")
@

Herefter er der `r nrow(train)` observationer tilbage og `r ncol(train)` attributter.

Der er for en del af attributterne en stor andel af manglende værdier. Det er især i forhold til variable fra BBR, som beskriver forskellige karakteristika ved selve boligen. Her har jeg har valgt at fjerne alle de attributter som har mere end 95\% manglende værdier.

<<echo=FALSE>>=
attribut_oversigt <- readRDS("atrtribut.rds")
attribut_oversigt <- attribut_oversigt[order(attribut_oversigt$pct_missing_values, decreasing = TRUE),]
knitr::kable(attribut_oversigt)
@

Som udgangspunkt vil jeg træne en model som skal være i stand til at kunne prædiktere værdien af et standard
parcelhus. Data som der skal trænes på skal derfor også være salg af standard parcelhuse.
Data er derfor blevet ensrettet på følgende måde:

% - enhed.antalvaerelser > 1 og enhed.antalvaerelser < 10
% - bolig_areal < 500 og bolig_areal > 50
% - bolig_alder > 0 og bolig_alder < 100
% - bygning.antaletager > 0 og bygning.antaletager < 4
% - enhed.antalbadevaerelser > 0 og enhed.antalbadevaerelser < 4
% - enhed.antalvandskylledetoiletter > 0 og enhed.antalvandskylledetoiletter < 4
% - fremskreven_pris_M2 < 30000 og fremskreven_pris_M2 > 0

Slutteligt er der blevet taget et valg om kun at udvælge de rå attributter som menes at være de
vigtigste i forhold til at forudsige værdien af et standad parcelhus.

<<echo=FALSE>>=
train <- readRDS("train_behandlet.rds")
attribut <- undersoeger_attributter(train)
knitr::kable(attribut)
@

 
De to attributter der dækker over tagtypematriale og ydervægsmateriale er diskrete variable som fordel
kan normaliseres med en one-out-of-k transformering.

Afstand til kyst og afstand til motorvej er to variable som jeg har valgt at binariserer. Det har jeg da de to
features forud for denne rapport er blevet imputeret. For afstand til kyst er afstanden op til 1500 meter målt.
Alt herover er imputeret til 1501 meter. Ligeledes er gjort for afstand til motorvej.
For at håndtere disse variable har jeg som sagt valgt at binariserer dem. For afstand til kyst har jeg ud fra et
forretningsmæssigt synspunkt valgt at en ejendom ligger så tæt på kysten at det har en effekt i dens værdi hvis
den ligger inden for 300 meter af kysten. Det samme er gjort for afstand til vej. Her er grænsen dog blot 100
meter.


<<echo=FALSE>>=
train <- one_out_of_k(train)
train <- binarisere_afstande(train)
@

\section{Data visualisering heriblandt Principal Component Analysis (PCA)}

Principal Component Analysis (PCA) er en metode som kan bruges til at reducere dimensionerne i det data der arbejdes med. Man kan have mange dimensioner i sit datasæt, men hvis de alle sammen er med til at forklare sammen tendens er det 'sande' antal af dimensioner lavere end antallet af attributter. Målet med at lave PCA er at reducere dimensionerne i data uden at reducere variationen, således at man ender op med data som med færre dimensioner men som kan forklare det samme som datasættet med flere dimensioner. PCA fungerer kun ud fra antagelsen om at der er en linear forklaring i data med færre dimensioner. De bedste projektion af data ned på et subspace er dem hvor observationer er spredt ud (Høj varians), men samtidig hvor residualerne reduceres. 
Vektoren bliver valgt ud fra at den skal være en eigenvektor til vores datamatrix som har den højeste eigenværdi. Singular Value Decomposition (SVD) er en metode som for en hvilken som helst $N*M$ matrix udregner eigenvektoren med den højeste eigenværdi.  

Ejendomsdata er blevet klargjort. Data er blevet tranformeret. Nogle variable er blevet tranformeret med one-out-of-K transformation, mens enkelte er blevet binariseret. 
Til PCA er det første trin at standardisere data, således at attributternes værdier er på samme skala. Selve standardiseringen består i at trække gennemsnittet fra hver attribut, hvorefter der også er blevet divideret med standardafvigelsen. For data betyder det at hver attribut reskaleres således at de får et gennemsnit på 0 og en standardafvigelse på 1. 
Årsagen til at en reduktion af dimensionerne er ønskværdig er at det for nogle typer af algoritmer kan være med til at forøge deres nøjagtighed. Dette er eksempelvis tilfældet med xgboost algoritmen. 

Efter alle datatransformationerne består data af 185018 observationer (N) med 23 features (M).
Dette data skal senere danne grundlaget for regressionsanalysen, men inden da bliver der med en korrelationsanalyse og en PCA taget stilling til hvorvidt det er muligt at reducere demensionerne i data. 
Resultatet af korrelationsanalysen er vist i et korrelationsplot. Resultatet af Korrelationsanalysen viser at der er en stor positiv korrelation mellem den fremskrevne kvadratmeter pris og den vægtede gennemsnitspris for de nærmeste naboer. Der er desuden også an større positiv sammenhæng mellem antallet af værelser og boligarealet. 
Disse to positive sammenhænge giver logik rigtig god mening. Salgspriser er i høj grad styret af det område som ejendommen ligger i. Ligger ejendommen i et dyrt område, vil naboerne blive solgt til høje handelspriser og det samme vil højst sandsynligt også gælde for den specifikke ejendom. 
Samtidig vil der typisk også være flere væresler jo større boligareal en ejendommene har. 

<<echo=FALSE, fig=TRUE>>=
train <- standardize(train)
correlation <- cor(train)
print(colnames(correlation))
rownames(correlation) <- 1:23
colnames(correlation) <- 1:23
corrplot(correlation)
@

Som en del af PCA udregnes herefter Singular Value Decomposition (SVD).

<<echo = FALSE, fig=TRUE>>=
# Lave singular value decomposition analyse:
svd <- svd(train)
svd_u <- svd$u
svd_v <- svd$v
svd_sigma <- diag(svd$d)
eigenvalues <- diag(svd_sigma^2/(nrow(train) - 1))
variance <- eigenvalues * 100 /sum(eigenvalues)
cumvar <- cumsum(variance)

plot(variance,
     type = "o",
     pch = 16,
     main = "Varians forklaret med principal components",
     xlab = "Principal components",
     ylab = "Forkalret varians",
     col = "deepskyblue")

plot(cumvar,
     type = "o",
     pch = 16,
     main = "Varians forklaret med principal components",
     xlab = "Principal components",
     ylab = "Kumulativ varians",
     col = "darkorchid")
@

De foeste 16 principal components kan forklare 90\% af variationen i data. For at kommme over 95\% skal man have de 18 første komponenter. Ud af de i alt 23 mulige komponenter er det med dette data ikke muligt at reducerer mange komponenter væk uden også at miste variation i data. 
Ved at have antallet af komponenter som er mindre end antallet af attributter i ens datasæt bliver information tabt, og hvorvidt man med fordel kan bruge PCA skal bestemmes ud fra den pågældende problemstilling. I den videre opgave har jeg valgt at gå videre med mit originale datasæt som det så ud før PCA. 

<<echo=FALSE, fig= TRUE>>=
z <- svd_u %*% svd_sigma
# plot(z[, 3 ], z[, 6])
@


\chapter{Part II}

\section{Regression - part A}
I part II er formålet at bruge det rensede data fra part I til at forudsige fremskrevne handelspriser ud fra forskellige varibable. 
Håbet med denne regressionsanalyse er at man ud fra relativt få variable og en relativt simpel model vil kunne forudsige ejendomspriserne. 
Forud for regressionsanalysen er data blevet tranformeret. For faktorvariablene tagtype og vægmateriale har jeg valgt at tranformere med en one-of-k transformering. Herefter er alle attributer blevet standardiseret, således at de har en gennemsnit på 0 og en standardafvigelse på 1. 

Den første lineære model der fittes er en univariate linear regressionsmodel. Her er naboernes områdepris den eneste variabel som bruges til at forudsige de fremskrevne handelspriser. 
Denne simple lineaære regression er vist i figuren nedenfor. 

<<echo=FALSE, fig=TRUE>>=
# Estimate model parameters
w_est = lm(fremskreven_pris_M2 ~ EV_NN_M2, data = train)
# Plot the predictions of the model
plot(train$fremskreven_pris_M2, train$EV_NN_M2,
     main='Simpel lineær regression',
     xlab="Naboernes kvadratmeterpriser", 
     ylab="Fremskrevne kvadratmeterpriser");
y_est = w_est$coef[1] +w_est$coef[2]*train$fremskreven_pris_M2;
lines(train$fremskreven_pris_M2, y_est, col='red');
legend("topleft", legend=c("Data", "Fittet model"), fill=c("black", "red"))
@

Den anden lineære model der fittes er en multivariate lineær regressionsmodel. 
Variablene som bliver brugt i modellen er områdepriser i form af naboernes kvadratmeterpriser. Det er boligens opførelsesalder og ombygningsår, og det er boligens og grundens areal. 
I en multivariate lineær regressionsmodel kan man ikke på samme måde plotte den fittede model på det todimensionelle plot. Her kan man i stedet estimerer vide hvor godt modellen fitter til data ved akan man ved at minimere summen af de kvadrerede afvigelser (RSS). 
Der findes flere forskellige typer af algoritmer hvis formål er at finde de parametre/vægte som laver det bedste fit til data ved at minimerer 'cost'. 

<<echo=FALSE, fig=TRUE>>=
# Estimate model parameters
fit = lm(fremskreven_pris_M2 ~ EV_NN_M2 + bolig_areal + aux.ice_info.jordstykker.registreretareal_fratrukket_vejareal + bolig_alder + bolig_alder, data = train)

train$predicted <- predict(fit)   # Save the predicted values
train$residuals <- residuals(fit) # Save the residual values

train %>% select(fremskreven_pris_M2, predicted, residuals) %>% head()

#Plot the predictions of the model
par(mfrow = c(2, 2))
plot(fit)

print(summary(fit))

# Make a scatter plot of predicted versus true values of Alcohol
plot(train$fremskreven_pris_M2, fit$fitted.values, 
     xlab='fremskreven_pris (true)', 
     ylab='fremskreven_pris (estimated)',
     main='salgspriser',
     pch=16,
     col = "deepskyblue")

# Make a histogram of the residual error
hist(train$fremskreven_pris_M2-fit$fitted.values,
     breaks=41,
     main="Residual error")

# Compute mean squared error
mean((train$fremskreven_pris_M2-fit$fitted.values)^2)
@

Evalueringen af den lineære model kan foregå ved at man træner på et datasæt og prædikterer på et andet. I følgende regression har jeg nabokvadratmeterpriserne og boligarealet med som prediktorvariable. 
Ud fra den estimerede model kan man plotte sin afhængige variabel mod de predikterede værdier.
Forskellen mellem den faktiske afhænge variabel (i dette tilfælde de fremskrevne salgspiser) og den prædikterede variabel (modellens estimerede y) er residualet.
Selve den lineære regressionsmodel kan anvanceres ved at tranformere inputvariablene. Det kan eksempelvis være at opløfte variablene i anden potens og bruge dem som prediktor sammen med den originale version af variablen.
Målet for denne simple lineære regressionsmodel er reducere 'cost-funktionen' så meget som muligt. For en simpel lineær regressiosmodel er cost-funktionen en squared error, hvilket vil sige at målet her er at reducere 'mean squared error' så meget som muligt - dog uden at overfitte. 
Andre cost-funktioner kan bruges til andre problemstillinger. En logistisk funktion kan således bruges til at hvis man arbejder med et logistisk regressionsproblem. 

<<echo=FALSE, fig=TRUE>>=
# x1 <- train[1:80000, ] 
# x2 <- train[80000:185018, ]
# 
# dat <- data.frame(x1[, c("EV_NN_M2","bolig_areal")])
# xnam <- paste("X", 1:2, sep="")
# colnames(dat) <- xnam
# y <- train$fremskreven_pris_M2[1:80000]
# (fmla <- as.formula(paste("y ~ ", paste(xnam, collapse= "+"))))
# w_est = lm(fmla, data=dat);
# 
# # Plot the predictions of the model
# plot(train$EV_NN_M2, train$fremskreven_pris_M2, main="Linear regression", xlab="X", ylab="y")
# 
# newdat <- data.frame(x2[, c("EV_NN_M2","bolig_areal")])
# colnames(newdat) <- xnam
# y_est = predict(w_est, newdata=newdat)
# lines(newdat[,1], y_est, col='red')
# legend("topleft", legend=c('Data', 'Fitted model'), fill=c("black", "red"))
@

Ridge regression - ridge regression er regressionsmetode som bruges for at undgå at man overfitter. Overfitting sker når den fittede model fitter træningsdata rigtig godt, men at den model der trænes ikke generalisere godt på ukendt data. Overfitting kan ske både hvis der er for mange prædiktor variable eller for få observationer. 
Et godt fit er kendetegnet ved et lav RSS, men også størrelsen på koeffiecienterne - disse to til sammen udgør den samlede 'cost'. I ridge regression bruges l2-normen som et mål for størrelsen på koefficienterne og målet med ridge regression er minimere den samlede 'cost'.
Dette kan gøres ved at indføre en regulariseringsparameter (lambda), og den kan bruges til at styre kompleksisteten af sin model. 

<<>>=
y <- train$fremskreven_pris_M2
x <- train %>% select(EV_NN_M2, bolig_areal, aux.ice_info.jordstykker.registreretareal_fratrukket_vejareal, bolig_alder, bolig_alder) %>% data.matrix()
lambdas <- 10^seq(3, -3, by = -.2)

# fit <- glmnet(x, y, alpha = 0, lambda = lambdas)
# summary(fit)
# cv_fit <- cv.glmnet(x, y, alpha = 0, lambda = lambdas)
# plot(cv_fit)
@

\section{Regression - part B}

\section{Classification}

\chapter{Part III}


\end{document}